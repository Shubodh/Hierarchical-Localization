{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "\n",
    "from hloc import extract_features, match_features, localize_inloc, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for indoor localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here we declare the paths to the dataset, image pairs, and we choose the feature extractor and the matcher. You need to download the [InLoc dataset](https://www.visuallocalization.net/datasets/) and put it in `datasets/inloc/`, or change the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_name = 'i5noydFURQK' #8WUmhLawc2A, 'EDJbREhghzL', 'i5noydFURQK', 'jh4fc5c5qoQ', 'mJXqzFtmKg4'\n",
    "# Set whether you want output for 2 queries or 2nd query or 1st query (small)\n",
    "query_2 = '_only2query.txt'; queries_2 = '_2queries.txt'; small = '_small.txt'\n",
    "h5_suffix = small\n",
    "\n",
    "# change this if your dataset is somewhere else\n",
    "dataset = Path('datasets/graphVPR/mp3d_' + scene_name + '_small/')\n",
    "pairs = Path('pairs/graphVPR/mp3d_' + scene_name + '_small/')\n",
    "\n",
    "outputs = Path('outputs/graphVPR/mp3d_' + scene_name + '_small/')  # where everything will be saved\n",
    "# results = outputs / 'InLoc_hloc_superpoint+superglue_netvlad40.txt'  # the result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc_pairs = pairs / 'pairs-query-netvlad40.txt'  # top 40 retrieved by NetVLAD\n",
    "loc_pairs = pairs / ('pairs-query-mp3d_' + scene_name + h5_suffix)  # top 40 retrieved by NetVLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs for feature extractors:\n",
      "{'d2net-ss': {'model': {'multiscale': False, 'name': 'd2net'},\n",
      "              'output': 'feats-d2net-ss',\n",
      "              'preprocessing': {'grayscale': False, 'resize_max': 1600}},\n",
      " 'dir': {'model': {'name': 'dir'},\n",
      "         'output': 'global-feats-dir',\n",
      "         'preprocessing': {'resize_max': 1024}},\n",
      " 'netvlad': {'model': {'name': 'netvlad'},\n",
      "             'output': 'global-feats-netvlad',\n",
      "             'preprocessing': {'resize_max': 1024}},\n",
      " 'sift': {'model': {'name': 'sift'},\n",
      "          'output': 'feats-sift',\n",
      "          'preprocessing': {'grayscale': True, 'resize_max': 1600}},\n",
      " 'superpoint_aachen': {'model': {'max_keypoints': 4096,\n",
      "                                 'name': 'superpoint',\n",
      "                                 'nms_radius': 3},\n",
      "                       'output': 'feats-superpoint-n4096-r1024',\n",
      "                       'preprocessing': {'grayscale': True,\n",
      "                                         'resize_max': 1024}},\n",
      " 'superpoint_inloc': {'model': {'max_keypoints': 4096,\n",
      "                                'name': 'superpoint',\n",
      "                                'nms_radius': 4},\n",
      "                      'output': 'feats-superpoint-n4096-r1600',\n",
      "                      'preprocessing': {'grayscale': True, 'resize_max': 1600}},\n",
      " 'superpoint_max': {'model': {'max_keypoints': 4096,\n",
      "                              'name': 'superpoint',\n",
      "                              'nms_radius': 3},\n",
      "                    'output': 'feats-superpoint-n4096-rmax1600',\n",
      "                    'preprocessing': {'grayscale': True,\n",
      "                                      'resize_force': True,\n",
      "                                      'resize_max': 1600}}}\n",
      "Configs for feature matchers:\n",
      "{'NN-mutual': {'model': {'do_mutual_check': True, 'name': 'nearest_neighbor'},\n",
      "               'output': 'matches-NN-mutual'},\n",
      " 'NN-ratio': {'model': {'do_mutual_check': True,\n",
      "                        'name': 'nearest_neighbor',\n",
      "                        'ratio_threshold': 0.8},\n",
      "              'output': 'matches-NN-mutual-ratio.8'},\n",
      " 'NN-superpoint': {'model': {'distance_threshold': 0.7,\n",
      "                             'do_mutual_check': True,\n",
      "                             'name': 'nearest_neighbor'},\n",
      "                   'output': 'matches-NN-mutual-dist.7'},\n",
      " 'superglue': {'model': {'name': 'superglue',\n",
      "                         'sinkhorn_iterations': 50,\n",
      "                         'weights': 'outdoor'},\n",
      "               'output': 'matches-superglue'}}\n"
     ]
    }
   ],
   "source": [
    "# list the standard configurations available\n",
    "print(f'Configs for feature extractors:\\n{pformat(extract_features.confs)}')\n",
    "print(f'Configs for feature matchers:\\n{pformat(match_features.confs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one of the configurations for extraction and matching\n",
    "# you can also simply write your own here!\n",
    "feature_conf = extract_features.confs['d2net-ss'] #superpoint_inloc\n",
    "matcher_conf = match_features.confs['NN-mutual']\n",
    "# matcher_conf = match_features.confs['superglue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract local features for database and query images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/09/2021 12:51:00 INFO] Extracting local features with configuration:\n",
      "{'model': {'multiscale': False, 'name': 'd2net'},\n",
      " 'output': 'feats-d2net-ss',\n",
      " 'preprocessing': {'grayscale': False, 'resize_max': 1600}}\n",
      "[08/09/2021 12:51:00 INFO] Found 140 images in root datasets/graphVPR/mp3d_i5noydFURQK_small.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/140 [00:00<?, ?it/s]/home/shubodh/anaconda3/envs/hloc/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100%|██████████| 140/140 [05:12<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/09/2021 12:56:35 INFO] Finished exporting features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feature_path = extract_features.main(feature_conf, dataset, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('outputs/graphVPR/mp3d_i5noydFURQK_small/feats-d2net-ss.h5')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp3d_query\n",
      "descriptors\n",
      "<HDF5 dataset \"descriptors\": shape (512, 7202), type \"<f4\">\n",
      "image_size\n",
      "<HDF5 dataset \"image_size\": shape (2,), type \"<i8\">\n",
      "keypoints\n",
      "<HDF5 dataset \"keypoints\": shape (7202, 2), type \"<f4\">\n",
      "scores\n",
      "<HDF5 dataset \"scores\": shape (7202,), type \"<f4\">\n",
      "mp3d_reference\n",
      "descriptors\n",
      "<HDF5 dataset \"descriptors\": shape (512, 7202), type \"<f4\">\n",
      "image_size\n",
      "<HDF5 dataset \"image_size\": shape (2,), type \"<i8\">\n",
      "keypoints\n",
      "<HDF5 dataset \"keypoints\": shape (7202, 2), type \"<f4\">\n",
      "scores\n",
      "<HDF5 dataset \"scores\": shape (7202,), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "# Experimentation\n",
    "with h5py.File(feature_path, 'r') as hfile:\n",
    "    for key in hfile.keys():\n",
    "        print(key)\n",
    "        dset = hfile[\"/mp3d_query/i5noydFURQK/1_rgb-i5noydFURQK-bathroom1.png\"]\n",
    "        for key2 in dset.keys():\n",
    "            print(key2)\n",
    "            print(dset[key2])\n",
    "#         print(dset)\n",
    "#         matches0 = dset['matches0']\n",
    "#         m0_np = np.array(matches0)\n",
    "#         print(f\"m0_np.shape {m0_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the query images\n",
    "Here we assume that the localization pairs are already computed using image retrieval (NetVLAD). To generate new pairs from your own global descriptors, have a look at `hloc/pairs_from_retrieval.py`. These pairs are also used for the localization - see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/09/2021 12:56:35 INFO] Matching local features with configuration:\n",
      "{'model': {'do_mutual_check': True, 'name': 'nearest_neighbor'},\n",
      " 'output': 'matches-NN-mutual'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [02:08<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/09/2021 12:58:44 INFO] Finished exporting matches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "match_path = match_features.main(matcher_conf, loc_pairs, feature_conf['output'], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('outputs/graphVPR/mp3d_i5noydFURQK_small/feats-d2net-ss_matches-NN-mutual_pairs-query-mp3d_i5noydFURQK_small.h5')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localize!\n",
    "Perform hierarchical localization using the precomputed retrieval and matches. Different from when localizing with Aachen, here we do not need a 3D SfM model here: the dataset already has 3D lidar scans. The file `InLoc_hloc_superpoint+superglue_netvlad40.txt` will contain the estimated query poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#localize_inloc.main(\n",
    "#    dataset, loc_pairs, feature_path, match_path, results,\n",
    "#    skip_matches=20)#20  # skip database images with too few matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "We parse the localization logs and for each query image plot matches and inliers with a few database images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization.visualize_loc(results, dataset, n=1, top_k_db=1, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "f945207f07e472c35fb5383c9fd2ca814e377077ca84d9dee2a2f4aad1f2e605"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
